{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder.appName(\"test spark datasource\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "# You can set the MASTER environment variable when running examples\n",
    "# to submit examples to a cluster.\n",
    "# This can be a mesos:// or spark:// URL, \"yarn\" to run on YARN,\n",
    "# and \"local\" to run locally with one thread,\n",
    "# or \"local[N]\" to run locally with N threads.\n",
    "\n",
    "df = spark.read.load(\"resources/users.parquet\")\n",
    "df.select(\"name\", \"favorite_color\") \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .save(\"namesAndFavColors.parquet\")\n",
    "df.show()\n",
    "\n",
    "## Manually Specifying Options\n",
    "# (json, parquet, jdbc, orc, libsvm, csv, text)\n",
    "df = spark.read.load(\"resources/people.json\",format=\"json\")\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .save(\"namesAndAges.parquet\",format=\"parquet\")\n",
    "\n",
    "\n",
    "# csv\n",
    "peopleDF = spark.read.load(\"resources/people.csv\"\n",
    "                     ,format=\"csv\",sep=\";\",inferSchema=\"true\",header=\"true\")\n",
    "peopleDF.show()\n",
    "\n",
    "# orc\n",
    "# The extra options are also used during write operation. For example, you can control bloom filters and dictionary encodings for ORC data sources. The following ORC example will create bloom filter and use dictionary encoding only for favorite_color. For Parquet, there exists parquet.bloom.filter.enabled and parquet.enable.dictionary, too. To find more detailed information about the extra ORC/Parquet options, visit the official Apache ORC / Parquet websites.\n",
    "df = spark.read.orc(\"resources/users.orc\")\n",
    "\n",
    "df.write.format(\"orc\")\\\n",
    "    .option(\"orc.bloom.filter.columns\",\"favorite_color\") \\\n",
    "    .option(\"orc.dictionary.key.threshold\",\"1.0\") \\\n",
    "    .option(\"orc.column.encoding.direct\",\"name\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"users_with_options.orc\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Run SQL on files directly\n",
    "\n",
    "df = spark.sql(\"select * from parquet.`resources/users.parquet`\")\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Save Modes\n",
    "\n",
    "# Scala/Java\tAny Language\tMeaning\n",
    "# SaveMode.ErrorIfExists (default)\t\"error\" or \"errorifexists\" (default)\tWhen saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.\n",
    "# SaveMode.Append\t\"append\"\tWhen saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.\n",
    "# SaveMode.Overwrite\t\"overwrite\"\tOverwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.\n",
    "# SaveMode.Ignore\t\"ignore\"\tIgnore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected not to save the contents of the DataFrame and not to change the existing data. This is similar to a CREATE TABLE IF NOT EXISTS in SQL.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Saving to Persistent Tables\n",
    "\n",
    "# DataFrames can also be saved as persistent tables into Hive metastore using the saveAsTable command. Notice that an existing Hive deployment is not necessary to use this feature. Spark will create a default local Hive metastore (using Derby) for you\n",
    "# For file-based data source, e.g. text, parquet, json, etc. you can specify a custom table path via the path option, e.g. df.write.option(\"path\", \"/some/path\").saveAsTable(\"t\"). When the table is dropped, the custom table path will not be removed and the table data is still there. If no custom table path is specified, Spark will write data to a default table path under the warehouse directory. When the table is dropped, the default table path will be removed too.\n",
    "\n",
    "# Starting from Spark 2.1, persistent datasource tables have per-partition metadata stored in the Hive metastore. This brings several benefits:\n",
    "\n",
    "# Since the metastore can return only necessary partitions for a query, discovering all the partitions on the first query to the table is no longer needed.\n",
    "# Hive DDLs such as ALTER TABLE PARTITION ... SET LOCATION are now available for tables created with the Datasource API.\n",
    "# Note that partition information is not gathered by default when creating external datasource tables (those with a path option). To sync the partition information in the metastore, you can invoke MSCK REPAIR TABLE."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "## Bucketing, Sorting and Partitioning\n",
    "\n",
    "# For file-based data source, it is also possible to bucket and sort or partition the output. Bucketing and sorting are applicable only to persistent tables:\n",
    "# SQL = CREATE TABLE users_bucketed_by_name(\n",
    "# name STRING,\n",
    "# favorite_color STRING,\n",
    "# favorite_numbers array<integer>\n",
    "# ) USING parquet\n",
    "# CLUSTERED BY(name) INTO 42 BUCKETS;\n",
    "peopleDF.write.mode(\"overwrite\") \\\n",
    "    .bucketBy(24,\"name\") \\\n",
    "    .sortBy(\"age\") \\\n",
    "    .saveAsTable(\"people_bucketed\")\n",
    "\n",
    "# while partitioning can be used with both save and saveAsTable when using the Dataset APIs\n",
    "# SQL = CREATE TABLE users_by_favorite_color(\n",
    "# name STRING,\n",
    "# favorite_color STRING,\n",
    "# favorite_numbers array<integer>\n",
    "# ) USING csv PARTITIONED BY(favorite_color);\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"favorite_color\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .save(\"namesPartByColor.parquet\")\n",
    "\n",
    "# It is possible to use both partitioning and bucketing for a single table:\n",
    "df = spark.read.parquet(\"resources/users.parquet\")\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"favorite_color\") \\\n",
    "    .bucketBy(42,\"name\") \\\n",
    "    .saveAsTable(\"users_partitioned_bucketed\")\n",
    "# partitionBy creates a directory structure as described in the Partition Discovery section. Thus, it has limited applicability to columns with high cardinality. In contrast bucketBy distributes data across a fixed number of buckets and can be used when the number of unique values is unbounded.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}