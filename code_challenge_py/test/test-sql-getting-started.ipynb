{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.version= 3.2.1\n"
     ]
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "\n",
    "# Starting Point: SparkSession\n",
    "# __requires__=\"pyspark==3.1.2\"\n",
    "# import pkg_resources\n",
    "# pkg_resources.require(\"pyspark==3.1.2\")\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"test python spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"spark.version=\",spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "| age|    name|\n",
      "+----+--------+\n",
      "|null|    test|\n",
      "|   5|     Ali|\n",
      "|   5|Mohammad|\n",
      "|  39|  Narges|\n",
      "|  46|    Adel|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrames\n",
    "df = spark.read.json(\"D:/workspace/code_challenge/datalake/people.json\")\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|    test|\n",
      "|     Ali|\n",
      "|Mohammad|\n",
      "|  Narges|\n",
      "|    Adel|\n",
      "+--------+\n",
      "\n",
      "+--------+---------+\n",
      "|    name|(age + 1)|\n",
      "+--------+---------+\n",
      "|    test|     null|\n",
      "|     Ali|        6|\n",
      "|Mohammad|        6|\n",
      "|  Narges|       40|\n",
      "|    Adel|       47|\n",
      "+--------+---------+\n",
      "\n",
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 39|Narges|\n",
      "| 46|  Adel|\n",
      "+---+------+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  39|    1|\n",
      "|null|    1|\n",
      "|   5|    2|\n",
      "|  46|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Untyped Dataset Operations (aka DataFrame Operations)\n",
    "\n",
    "# Print the schema in a tree format\n",
    "df.printSchema()\n",
    "\n",
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show()\n",
    "\n",
    "# Select everybody, but increment the age by 1\n",
    "df.select(df[\"name\"],df[\"age\"]+1).show()\n",
    "\n",
    "# Select people older than 21\n",
    "df.filter(df[\"age\"]>21).show()\n",
    "\n",
    "# Count people by age\n",
    "df.groupby(\"age\").count().show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "| age|    name|\n",
      "+----+--------+\n",
      "|null|    test|\n",
      "|   5|     Ali|\n",
      "|   5|Mohammad|\n",
      "|  39|  Narges|\n",
      "|  46|    Adel|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Running SQL Queries Programmatically\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sql_df = spark.sql(\"select * from people\")\n",
    "sql_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "| age|    name|\n",
      "+----+--------+\n",
      "|null|    test|\n",
      "|   5|     Ali|\n",
      "|   5|Mohammad|\n",
      "|  39|  Narges|\n",
      "|  46|    Adel|\n",
      "+----+--------+\n",
      "\n",
      "+----+--------+\n",
      "| age|    name|\n",
      "+----+--------+\n",
      "|null|    test|\n",
      "|   5|     Ali|\n",
      "|   5|Mohammad|\n",
      "|  39|  Narges|\n",
      "|  46|    Adel|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Global Temporary View\n",
    "\n",
    "# Register the DataFrame as a global temporary view\n",
    "df.createOrReplaceGlobalTempView(\"people\")\n",
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"select * from global_temp.people\").show()\n",
    "\n",
    "# Global temporary view is cross-session\n",
    "spark.newSession().sql(\"select * from global_temp.people\").show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "## Creating Datasets\n",
    "\n",
    "# Datasets are similar to RDDs, however, instead of using Java serialization or Kryo\n",
    "# they use a specialized Encoder to serialize the objects for processing or transmitting\n",
    "# over the network. While both encoders and standard serialization are responsible\n",
    "# for turning an object into bytes, encoders are code generated dynamically\n",
    "# and use a format that allows Spark to perform many operations like filtering, sorting\n",
    "# and hashing without deserializing the bytes back into an object.\n",
    "# SCALA/JAVA\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:mohammad\n",
      "name:ali\n"
     ]
    }
   ],
   "source": [
    "## Interoperating with RDDs\n",
    "\n",
    "## Inferring the Schema Using Reflection\n",
    "\n",
    "from pyspark.sql import Row\n",
    "sc = spark.sparkContext\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"D:/workspace/code_challenge/datalake/people.txt\")\n",
    "# parts = lines.map(lambda l: l.split(\",\"))\n",
    "# people = parts.map(lambda p:Row(name=p[0],age=int(p[1])))\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=(int(p[1]) if len(p[1])>0 else None)))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "df_people = spark.createDataFrame(people)\n",
    "# EX: Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
    "# ANS : set PYSPARK_PYTHON=python\n",
    "df_people.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "kids = spark.sql(\"select * from people where age<10 and age>3\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "kidNames = kids.rdd.map(lambda k: \"name:\"+k.name).collect()\n",
    "for name in kidNames:\n",
    "    print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Programmatically Specifying the Schema\n",
    "\n",
    "# Import data types\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "sc = spark.sparkContext\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"D:/workspace/code_challenge/datalake/people.txt\")\n",
    "parts = lines.map(lambda l:l.split(\",\"))\n",
    "# Each line is converted to a tuple.\n",
    "people = parts.map(lambda p:(p[0],p[1].strip()))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"name age\"\n",
    "\n",
    "fields = [StructField(field_name,StringType(),True) for field_name in schemaString.split(\" \")]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaPeople = spark.createDataFrame(people,schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"select name from people\")\n",
    "\n",
    "results.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|    test|\n",
      "|mohammad|\n",
      "|     ali|\n",
      "|  narges|\n",
      "|    adel|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Scalar Functions\n",
    "\n",
    "# Scalar functions are functions that return a single value per row, as opposed to aggregation functions, which return a value for a group of rows. Spark SQL supports a variety of Built-in Scalar Functions. It also supports User Defined Scalar Functions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Aggregate Functions\n",
    "\n",
    "# Aggregate functions are functions that return a single value on a group of rows. The Built-in Aggregation Functions provide common aggregations such as count(), count_distinct(), avg(), max(), min(), etc. Users are not limited to the predefined aggregate functions and can create their own. For more details about user defined aggregate functions, please refer to the documentation of User Defined Aggregate Functions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}