{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import stat\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from pyspark import Row, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "#,functions\n",
    "# from pyspark import SparkConf\n",
    "# conf = SparkConf()\n",
    "# conf.set(\"spark.master\",\"local[*]\") \\\n",
    "#     .set(\"spark.app.name\",\"spark-advance\") \\\n",
    "#     .set(\"spark.storage.memoryFraction\", \"0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder.appName(\"spark-advance\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "print(\"spark session built.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65973\n"
     ]
    }
   ],
   "source": [
    "# load json from relative path\n",
    "df = spark.read\\\n",
    "    .format(\"json\")\\\n",
    "    .option(\"pathGlobFilter\",\"*.json\")\\\n",
    "    .load(\"../DataLake/searches/\")\n",
    "print(df.count())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#save parquet in specific path\n",
    "df.write\\\n",
    "    .format(\"parquet\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(\"../parquet/searches\")\n",
    "print(\"save parquet succeed.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nno viable alternative at input 'parquet.\"./searches\"'(line 1, pos 22)\n\n== SQL ==\nselect * from parquet.\"./searches\"\n----------------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mParseException\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_21516\\2628798886.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m#sql parquet in specific path\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'select * from parquet.\"./searches\"'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;31m#we cant specify relative path in spark.sql\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\session.py\u001B[0m in \u001B[0;36msql\u001B[1;34m(self, sqlQuery)\u001B[0m\n\u001B[0;32m    721\u001B[0m         \u001B[1;33m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row1'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row2'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row3'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    722\u001B[0m         \"\"\"\n\u001B[1;32m--> 723\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_wrapped\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    724\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    725\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mtable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtableName\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1319\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1321\u001B[1;33m         return_value = get_return_value(\n\u001B[0m\u001B[0;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0;32m   1323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    115\u001B[0m                 \u001B[1;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m                 \u001B[1;31m# JVM exception message.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 117\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    118\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m                 \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mParseException\u001B[0m: \nno viable alternative at input 'parquet.\"./searches\"'(line 1, pos 22)\n\n== SQL ==\nselect * from parquet.\"./searches\"\n----------------------^^^\n"
     ]
    }
   ],
   "source": [
    "#sql parquet in specific path XXXXXXXXXXXXXXXXX\n",
    "df = spark.sql('select * from parquet.\"../parquet/searches\"')\n",
    "print(df.count())\n",
    "#we cant specify relative path in spark.sql XXXXXXXXXXXXX"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save hive succeed.\n"
     ]
    }
   ],
   "source": [
    "#save in hive\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"searches\")\n",
    "print(\"save hive succeed.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save hive succeed.\n"
     ]
    }
   ],
   "source": [
    "#save hive in specific path\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\",\"../hive/\") \\\n",
    "    .saveAsTable(\"searches\")\n",
    "print(\"save hive in specific path succeed.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65973\n",
      "save hive succeed.\n"
     ]
    }
   ],
   "source": [
    "#query hive specific\n",
    "df = spark.sql(\"select * from searches\")\n",
    "print(df.count())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `parquet`.`config`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Ignore\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_21516\\566062483.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m#create table config succeed.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"create table if not exists parquet.config(key string,name string)\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"create table config succeed.\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\session.py\u001B[0m in \u001B[0;36msql\u001B[1;34m(self, sqlQuery)\u001B[0m\n\u001B[0;32m    721\u001B[0m         \u001B[1;33m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row1'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row2'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row3'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    722\u001B[0m         \"\"\"\n\u001B[1;32m--> 723\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_wrapped\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    724\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    725\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mtable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtableName\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1319\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1321\u001B[1;33m         return_value = get_return_value(\n\u001B[0m\u001B[0;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0;32m   1323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    115\u001B[0m                 \u001B[1;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m                 \u001B[1;31m# JVM exception message.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 117\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    118\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m                 \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAnalysisException\u001B[0m: Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `parquet`.`config`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Ignore\n"
     ]
    }
   ],
   "source": [
    "#create table config succeed.\n",
    "spark.sql(\"create table if not exists parquet.config(key string,name string)\")\n",
    "print(\"create table config succeed.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Modified Time :  Mon Feb 14 10:42:00 2022\n",
      "1644856071.685726-searches-part-00004-20200126.json\n",
      "1644856071.685726\n",
      "<class 'datetime.datetime'>\n",
      "2022-02-14T19:57:51\n"
     ]
    }
   ],
   "source": [
    "# find max_modified_date in files of path\n",
    "fileStatsObj = os.stat( \"../DataLake/searches/\" )\n",
    "modificationTime = time.ctime ( fileStatsObj [ stat.ST_MTIME ] )\n",
    "print(\"Last Modified Time : \", modificationTime )\n",
    "\n",
    "max_filename = \"\"\n",
    "max_time =0\n",
    "for root, dirs, files in os.walk('../DataLake/searches/'):\n",
    "    max_filename = max(max_filename, max(str(os.stat(root+\"/\"+file).st_mtime)+\"-\"+file for file in files) )\n",
    "    max_time = max(max_time, max(os.stat(root+\"/\"+file).st_mtime for file in files) )\n",
    "print(max_filename)\n",
    "print(max_time)\n",
    "t = datetime.datetime.fromtimestamp(max_time)\n",
    "print(type(t))\n",
    "print(t.strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "# print(time.ctime(max(os.path.getmtime(file) for file in files)))\n",
    "# print(time.ctime(max(os.stat(file).st_mtime for file in files)))\n",
    "# print(max(str(os.stat(file).st_mtime)+\"-\"+file for file in files))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists\n",
      "+------------------+-------------------+\n",
      "|               key|              value|\n",
      "+------------------+-------------------+\n",
      "|last_modified_date|2021-05-25T19:58:04|\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df=(spark.read.format(\"parquet\").load(\"config\"))\n",
    "    print(\"exists\")\n",
    "    df.show()\n",
    "except:\n",
    "    print(\"not\")\n",
    "    rdd = [Row(key='last_modified_date', value='2021-05-25T19:58:04')]\n",
    "    df = spark.createDataFrame(rdd)\n",
    "    df.write.parquet(\"config\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|              value|\n",
      "+-------------------+\n",
      "|2021-05-25T19:58:04|\n",
      "+-------------------+\n",
      "\n",
      "2021-05-25T19:58:04\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"insert into parquet.config values ('searches.last_file_date','2021-05-25T19:58:04')\")\n",
    "# read last modified date\n",
    "df = spark.sql(\"select value from parquet.config where key='last_modified_date'\")\n",
    "df.show()\n",
    "print(df.collect()[0][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65973\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"insert into parquet.config values ('searches.last_file_date','2021-05-25T19:58:04')\")\n",
    "\n",
    "#read new ones\n",
    "df = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"pathGlobFilter\",\"*.json\") \\\n",
    "    .load(\"../DataLake/searches/\", modifiedAfter=datetime.datetime(2021,5,26))\n",
    "print(df.count())\n",
    "#     .load(\"../DataLake/searches/\", modifiedAfter=datetime.datetime() \"2021-05-25T19:58:04\")\n",
    "# 2021-05-25T19:58:04\n",
    "# .option(\"modifiedAfter\",\"2021-05-25T19:58:04\")\n",
    "# 2021-05-25T19:58:04\n",
    "# 65,973\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hive spark session built.\n"
     ]
    }
   ],
   "source": [
    "# create sparksession on hive\n",
    "spark = SparkSession\\\n",
    "    .builder.appName(\"test\")\\\n",
    "    .master(\"local\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\",\"../spark-warehouse\",SparkConf())\\\n",
    "    .getOrCreate()\n",
    "print(\"hive spark session built.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write test table succed\n"
     ]
    }
   ],
   "source": [
    "# saveAsTable in hive storage\n",
    "rdd = [Row(key='k1', value='value1')\n",
    "    ,Row(key='k2', value='value2')\n",
    "    ,Row(key='k3', value='value3')]\n",
    "df = spark.createDataFrame(rdd)\n",
    "df.write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"parquet\")\\\n",
    "    .saveAsTable(\"test\")\n",
    "print(\"write test table succed\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: test;\n'UnresolvedRelation [test], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_25996\\2419417323.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# read table on hive\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"test\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\session.py\u001B[0m in \u001B[0;36mtable\u001B[1;34m(self, tableName)\u001B[0m\n\u001B[0;32m    739\u001B[0m         \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    740\u001B[0m         \"\"\"\n\u001B[1;32m--> 741\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtableName\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_wrapped\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    742\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    743\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1319\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1321\u001B[1;33m         return_value = get_return_value(\n\u001B[0m\u001B[0;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0;32m   1323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    115\u001B[0m                 \u001B[1;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m                 \u001B[1;31m# JVM exception message.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 117\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    118\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m                 \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAnalysisException\u001B[0m: Table or view not found: test;\n'UnresolvedRelation [test], [], false\n"
     ]
    }
   ],
   "source": [
    "# read table on hive\n",
    "df = spark.table(\"test\")\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: test; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [test], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_25996\\965533486.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# query table on hive\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"select * from test\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\session.py\u001B[0m in \u001B[0;36msql\u001B[1;34m(self, sqlQuery)\u001B[0m\n\u001B[0;32m    721\u001B[0m         \u001B[1;33m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row1'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row2'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row3'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    722\u001B[0m         \"\"\"\n\u001B[1;32m--> 723\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_wrapped\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    724\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    725\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mtable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtableName\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1319\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1321\u001B[1;33m         return_value = get_return_value(\n\u001B[0m\u001B[0;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0;32m   1323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    115\u001B[0m                 \u001B[1;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m                 \u001B[1;31m# JVM exception message.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 117\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    118\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m                 \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAnalysisException\u001B[0m: Table or view not found: test; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [test], [], false\n"
     ]
    }
   ],
   "source": [
    "# query table on hive\n",
    "df = spark.sql(\"select * from test\")\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# write config to file\n",
    "import json\n",
    "conf = {\"k1\":\"value1\"\n",
    "    ,\"k2\":\"value2\"\n",
    "    ,\"k3\":\"value3\"}\n",
    "with open(\"config.json\", \"w\") as configfile:\n",
    "    json.dump(conf, configfile)\n",
    "    configfile.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k1': 'value1', 'k2': 'value2', 'k3': 'value3'}\n"
     ]
    }
   ],
   "source": [
    "# read config from json file\n",
    "with open(\"config.json\", \"r\") as configfile:\n",
    "    conf = json.load(configfile)\n",
    "print(conf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}